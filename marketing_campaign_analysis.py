# -*- coding: utf-8 -*-
"""Marketing_Campaign_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uKZ23ek-avnS3M4tKHFfckP-bAkaky3e

# **Marketing Campaign Analysis**

## **Problem Definition**

### **The Context:**

 - Customer segmentation is crucial for optimizing marketing strategies and improving the return on investment (ROI). By understanding customer behavior and characteristics, businesses can create personalized marketing campaigns that cater to the specific needs and interests of different customer groups. This approach can significantly enhance customer engagement, increase sales, and improve overall customer satisfaction. In today's competitive market, personalized communication and targeted marketing are essential for maintaining a competitive edge. Understanding customer segmentation helps businesses utilize their resources more efficiently by targeting specific groups with tailored marketing strategies. It can lead to higher engagement rates, increased sales, and improved customer retention. Effective customer segmentation allows businesses to address the unique needs and preferences of different customer groups, resulting in more effective marketing campaigns and better allocation of marketing budgets.

### **The objective:**

 - The primary goal is to identify the best possible customer segments using unsupervised learning techniques such as dimensionality reduction and clustering. By analyzing the given customer dataset, the objective is to group customers based on common characteristics, which can then be used to tailor marketing efforts and improve business outcomes.

### **The key questions:**

* What are the most significant factors that differentiate customer groups?
* How can we effectively segment the customer base using the provided dataset?
* What insights can be derived from the segmented customer groups to enhance marketing strategies?
* How can these customer segments be used to optimize marketing campaigns and improve ROI?




### **The problem formulation**:

- Using data science techniques, specifically unsupervised learning methods like clustering and dimensionality reduction, we aim to segment the customer base into distinct groups. These segments should reflect common characteristics and behaviors that can be leveraged to create targeted marketing campaigns. The analysis will involve exploring the dataset, identifying key features, applying clustering algorithms, and interpreting the results to derive actionable insights.

------------------------------
## **Data Dictionary**
------------------------------

The dataset contains the following features:

1. ID: Unique ID of each customer
2. Year_Birth: Customer’s year of birth
3. Education: Customer's level of education
4. Marital_Status: Customer's marital status
5. Kidhome: Number of small children in customer's household
6. Teenhome: Number of teenagers in customer's household
7. Income: Customer's yearly household income in USD
8. Recency: Number of days since the last purchase
9. Dt_Customer: Date of customer's enrollment with the company
10. MntFishProducts: The amount spent on fish products in the last 2 years
11. MntMeatProducts: The amount spent on meat products in the last 2 years
12. MntFruits: The amount spent on fruits products in the last 2 years
13. MntSweetProducts: Amount spent on sweet products in the last 2 years
14. MntWines: The amount spent on wine products in the last 2 years
15. MntGoldProds: The amount spent on gold products in the last 2 years
16. NumDealsPurchases: Number of purchases made with discount
17. NumCatalogPurchases: Number of purchases made using a catalog (buying goods to be shipped through the mail)
18. NumStorePurchases: Number of purchases made directly in stores
19. NumWebPurchases: Number of purchases made through the company's website
20. NumWebVisitsMonth: Number of visits to the company's website in the last month
21. AcceptedCmp1: 1 if customer accepted the offer in the first campaign, 0 otherwise
22. AcceptedCmp2: 1 if customer accepted the offer in the second campaign, 0 otherwise
23. AcceptedCmp3: 1 if customer accepted the offer in the third campaign, 0 otherwise
24. AcceptedCmp4: 1 if customer accepted the offer in the fourth campaign, 0 otherwise
25. AcceptedCmp5: 1 if customer accepted the offer in the fifth campaign, 0 otherwise
26. Response: 1 if customer accepted the offer in the last campaign, 0 otherwise
27. Complain: 1 If the customer complained in the last 2 years, 0 otherwise

**Note:** You can assume that the data is collected in the year 2016.

## **Important Notes**

- This notebook can be considered a guide to refer to while solving the problem. The evaluation will be as per the Rubric shared for the Milestone. Unlike previous courses, it does not follow the pattern of the graded questions in different sections. This notebook will give you a direction on what steps need to be taken in order to get a viable solution to the problem. Please note that this is just one way of doing this. There can be other 'creative' ways to solve the problem and we urge you to feel free and explore them as an 'optional' exercise.

- In the notebook, there are markdown cells called - Observations and Insights. It is a good practice to provide observations and extract insights from the outputs.

- The naming convention for different variables can vary. Please consider the code provided in this notebook as a sample code.

- All the outputs in the notebook are just for reference and can be different if you follow a different approach.

- There are sections called **Think About It** in the notebook that will help you get a better understanding of the reasoning behind a particular technique/step. Interested learners can take alternative approaches if they wish to explore different techniques.
"""



from google.colab import drive
drive.mount('/content/drive')

!pip install scikit-learn-extra

"""### **Loading Libraries**"""

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# To scale the data using z-score
from sklearn.preprocessing import StandardScaler

# To compute distances
from scipy.spatial.distance import cdist

# To perform K-means clustering and compute Silhouette scores
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# To visualize the elbow curve and Silhouette scores
from yellowbrick.cluster import SilhouetteVisualizer

# Importing PCA
from sklearn.decomposition import PCA

# To encode the variable
from sklearn.preprocessing import LabelEncoder

# Importing TSNE
from sklearn.manifold import TSNE

# To perform hierarchical clustering, compute cophenetic correlation, and create dendrograms
from sklearn.cluster import AgglomerativeClustering, DBSCAN
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

# To compute distances
from scipy.spatial.distance import pdist

# To import K-Medoids
from sklearn_extra.cluster import KMedoids

# To import Gaussian Mixture
from sklearn.mixture import GaussianMixture

# To supress warnings
import warnings

warnings.filterwarnings("ignore")

"""### **Let us load the data**"""

# loading the dataset
data = pd.read_csv("/content/drive/MyDrive/Datasets/marketing_campaign.csv")

"""### **Check the shape of the data**"""

# # Check the shape of the data
print("Shape of the data:", data.shape)

"""#### **The dataset consists of 2,240 rows and 27 columns, providing a substantial sample size for meaningful segmentation analysis. Each record has 27 attributes, indicating a rich dataset with a diverse range of features including demographic details, purchase behavior, and responses to marketing campaigns. With this number of attributes, we have enough information to perform detailed customer segmentation, analyze purchasing patterns, and assess campaign effectiveness.**

### **Understand the data by observing a few rows**
"""

#print("First 5 rows of the dataset:")
print(data.head())

# print("Last 5 rows of the dataset:")
print(data.tail())

"""#### **The dataset provides detailed customer information, which includes demographics, purchase history, and marketing engagement metrics. This comprehensive data allows for a thorough analysis and segmentation of customers. Observing the initial and final rows confirms the dataset's structure and completeness, indicating it is well-suited for segmentation analysis. The data appears consistent, which is crucial for accurate insights and reliable modeling.**

### **Let us check the data types and and missing values of each column**
"""

# # Check the datatypes of each column
print("Data types of each column:")
print(data.info())

# Find the percentage of missing values in each column
missing_values = data.isnull().mean() * 100
print("Percentage of missing values in each column:")
print(missing_values)

"""#### **Most columns in the dataset have no missing values, ensuring data completeness for analysis. The only column with missing values is Income, which has approximately 1.07% missing data. This low percentage of missing data indicates that the dataset is relatively complete and will not significantly impact the analysis. The data types are consistent with the expected types for each column, confirming the dataset's suitability for further processing and analysis.**

We can observe that `ID` has no null values. Also the number of unique values are equal to the number of observations. So, `ID` looks like an index for the data entry and such a column would not be useful in providing any predictive power for our analysis. Hence, it can be dropped.

**Dropping the ID column**
"""

# Remove ID column from data
data.drop('ID', axis=1, inplace=True)

"""## **Exploratory Data Analysis**

### **Let us now explore the summary statistics of numerical variables**
"""

# Explore basic summary statistics of numeric variables
print(data.describe())

"""#### **Observations and Insights: Summary Statistics of Numerical Variables. The summary statistics reveal significant variability in customer income, spending habits, and engagement with marketing campaigns. The average annual income is around 52,247 with a wide range, indicating diverse economic backgrounds. Spending on different product categories varies, with notable expenditures on meat products. Physical stores are a major purchase channel, as evidenced by the average of 5.8 store purchases. Online engagement is moderate, with an average of 5.3 web visits in the last month. Campaign acceptance rates are generally low, suggesting opportunities to enhance marketing strategies. These insights highlight the diversity in customer behavior and preferences, crucial for effective segmentation.**

### **Let us also explore the summary statistics of all categorical variables and the number of unique observations in each category**
"""

# List of the categorical columns in the data
cols = ["Education", "Marital_Status", "Kidhome", "Teenhome", "Complain"]

"""**Number of unique observations in each category**"""

# Number of unique observations in each category
for column in cols:
    print("Unique values in", column, "are :")
    print(data[column].value_counts())
    print("*" * 50)

"""#### **The dataset reveals diverse customer demographics: 50.3% of customers have a Graduation level education, while 21.7% have a PhD. Most customers are Married (38.6%) or Together (25.9%), with 21.4% being Single. A majority (57.7%) do not have small children, and 51.7% do not have teenagers. Only 1% of customers have made complaints. These quantifiable insights into education levels, marital status, and household composition are crucial for tailoring effective marketing strategies to different customer segments.**

**Think About It:**

- We could observe from the summary statistics of categorical variables that the Education variable has 5 categories. Are all categories different from each other or can we combine some categories? Is 2n Cycle different from Master?
- Similarly, there are 8 categories in Marital_Status with some categories having very low count of less than 5. Can we combine these categories with other categories?

### **Let us replace  the "2n Cycle" category with "Master" in Education and "Alone", "Absurd, and "YOLO" with "Single" in Marital_Status**
"""

# Replace the category "2n Cycle" with the category "Master"
data['Education'].replace({'2n Cycle': 'Master'}, inplace=True)

# Replace the categories "Alone", "Absurd", and "YOLO" with the category "Single"
data['Marital_Status'].replace({'Alone': 'Single', 'Absurd': 'Single', 'YOLO': 'Single'}, inplace=True)

"""## **Univariate Analysis**
Univariate analysis is used to explore each variable in a data set, separately. It looks at the range of values, as well as the central tendency of the values. It can be done for both numerical and categorical variables.

## **1. Univariate Analysis - Numerical Data**
Histograms help to visualize and describe numerical data. We can also use other plots like box plot to analyze the numerical columns.

#### Let us plot histogram for the feature 'Income' to understand the distribution and outliers, if any.
"""

# Create histogram for the Income feature

plt.figure(figsize=(15, 7))
sns.histplot(x='Income', data=data)
plt.show()

"""**We could observe some extreme value on the right side of the distribution of the 'Income' feature. Let's use a box plot as it is more suitable to identify extreme values in the data.**"""

# Plot the boxplot
sns.boxplot(data=data, x='Income', showmeans=True, color="violet")
plt.show()

"""#### **The histogram of the Income feature shows that most customer incomes are concentrated in the lower to mid-range, with a sharp decline as income increases, indicating most customers fall within a similar income bracket. The box plot confirms this distribution, highlighting significant high-income outliers. These insights into the income distribution are crucial for understanding the economic background of the customer base, essential for effective customer segmentation and tailored marketing strategies.**

**Think About It**

- The histogram and the box plot are showing some extreme value on the right side of the distribution of the 'Income' feature. Can we consider them as outliers and remove or should we analyze these extreme values?
"""

# Calculating the upper whisker for the Income variable
Q1 = data['Income'].quantile(q=0.25)  # Finding the first quartile
Q3 = data['Income'].quantile(q=0.75)  # Finding the third quartile
IQR = Q3 - Q1  # Finding the Inter Quartile Range

upper_whisker = Q3 + 1.5 * IQR  # Calculating the Upper Whisker for the Income variable
print(upper_whisker)  # Printing Upper Whisker

# Let's check the observations with extreme value for the Income variable
data[data.Income > upper_whisker]

"""**Think About It:**

- We observed that there are only a few rows with extreme values for the Income variable. Is that enough information to treat (or not to treat) them? Do we know at what percentile the upper whisker lies?
"""

# Check the 99.5% percentile value for the Income variable
percentile_995 = data['Income'].quantile(q=0.995)
print(percentile_995)

"""#### **The dataset contains extreme outliers in the Income variable, with values reaching up to 666,666. The 99.5th percentile for Income is approximately 102,146.75, indicating that most incomes fall below this threshold. These outliers can skew analysis and may require treatment to ensure accurate modeling and insights.**



"""

# Dropping observations identified as outliers
data.drop(index=data[data['Income'] > percentile_995].index, inplace=True)  # Pass the indices of the observations (separated by a comma) to drop them

"""**Now, let's check the distribution of the Income variable after dropping outliers.**"""

# Plot histogram and 'Income'
plt.figure(figsize=(15, 7))
sns.histplot(x='Income', data=data)
plt.show()

# Plot the histogram for 'MntWines'
plt.figure(figsize=(15, 7))
sns.histplot(x='MntWines', data=data)
plt.show()

# Plot the histogram for 'MntFruits'
plt.figure(figsize=(15, 7))
sns.histplot(x='MntFruits', data=data)
plt.show()

# Plot the histogram for 'MntMeatProducts'
plt.figure(figsize=(15, 7))
sns.histplot(x='MntMeatProducts', data=data)
plt.show()

# Plot the histogram for 'MntFishProduct'
plt.figure(figsize=(15, 7))
sns.histplot(x='MntFishProducts', data=data)
plt.show()

# Plot the histogram for 'MntSweetProducts'
plt.figure(figsize=(15, 7))
sns.histplot(x='MntSweetProducts', data=data)
plt.show()

# Plot the histogram for 'MntGoldProducts'
plt.figure(figsize=(15, 7))
sns.histplot(x='MntGoldProds', data=data)
plt.show()

"""#### **Note:** Try plotting histogram for different numerical features and understand how the data looks like.

#### **Observations and Insights for all the plots: _____**

## **2. Univariate analysis - Categorical Data**

Let us write a function that will help us create bar plots that indicate the percentage for each category. This function takes the categorical column as the input and returns the bar plot for the variable.
"""

def perc_on_bar(data, z):
    '''
    plot
    feature: categorical feature
    the function won't work if a column is passed in hue parameter
    '''

    total = len(data[z])                                          # Length of the column
    plt.figure(figsize=(15,5))
    ax = sns.countplot(data[z],palette='Paired',order = data[z].value_counts().index)
    for p in ax.patches:
        percentage = '{:.1f}%'.format(100 * p.get_height()/total) # Percentage of each class of the category
        x = p.get_x() + p.get_width() / 2 - 0.05                  # Width of the plot
        y = p.get_y() + p.get_height()                            # Height of the plot

        ax.annotate(percentage, (x, y), size = 12)                # Annotate the percentage

    plt.show()                                                    # Show the plot

"""#### Let us plot barplot for the variable Marital_Status."""

# Bar plot for 'Marital_Status'
perc_on_bar(data, 'Marital_Status')

"""#### **Note:** Explore for other categorical variables like Education, Kidhome, Teenhome, Complain.

#### **Observations and Insights from all plots: _____**

## **Bivariate Analysis**

We have analyzed different categorical and numerical variables. Now, let's check how different variables are related to each other.

### **Correlation Heat map**
Heat map can show a 2D correlation matrix between numerical features.
"""

# Select only numerical columns before calculating correlations
numerical_data = data.select_dtypes(include=['number'])

plt.figure(figsize=(15, 7))  # Setting the plot size
sns.heatmap(numerical_data.corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")  # Plotting the correlation plot
plt.show()

"""#### **Observations and Insights: _____**

**The above correlation heatmap only shows the relationship between numerical variables. Let's check the relationship of numerical variables with categorical variables.**

### **Education Vs Income**
"""

plt.figure(figsize=(10, 6))
sns.barplot(x='Education', y='Income', data=data)
plt.show()

"""#### **Observations and Insights: _____**

### **Marital Status Vs Income**
"""

# Plot the bar plot for Marital_Status and Income
plt.figure(figsize=(10, 6))
sns.barplot(x='Marital_Status', y='Income', data=data)
plt.show()

"""#### **Observations and Insights: _____**

### **Kidhome Vs Income**
"""

# Plot the bar plot for Kidhome and Income
plt.figure(figsize=(10, 6))
sns.barplot(x='Kidhome', y='Income', data=data)
plt.show()

"""#### **Observations and Insights: _____**

**We can also visualize the relationship between two categorical variables.**

### **Marital_Status Vs Kidhome**
"""

# Plot the bar plot for Marital_Status and Kidhome
pd.crosstab(data['Marital_Status'], data['Kidhome']).plot(kind='bar', stacked=False)
plt.show()

"""#### **Observations and Insights: _____**

## **Feature Engineering and Data Processing**

In this section, we will first prepare our dataset for analysis.
- Creating new columns
- Imputing missing values

**Think About It:**

- The Year_Birth column in the current format might not be very useful in our analysis. The Year_Birth column contains the information about Day, Month, and year. Can we extract the age of each customer?
- Are there other columns which can be used to create new features?

### **Age**
"""

# Extract only the year from the Year_Birth variable and subtracting it from 2016 will give us the age of the customer at the time of data collection in 2016
data["Age"] = 2016 - pd.to_datetime(data['Year_Birth'], format="%Y").apply(lambda x: x.year)

# Sorting the values in ascending order
data["Age"].sort_values()

"""#### **Observations and Insights: _____**

**Think About It:**

- We could observe from the above output that there are customers with an age greater than 115. Can this be true or a data anomaly? Can we drop these observations?
"""

# Drop the observations with age > 115
# Hint: Use drop() method with inplace=True
data.drop(index=data[data['Age'] > 115].index, inplace=True)  # Hint: Use drop() method with inplace=True

"""**Now, let's check the distribution of age in the data.**"""

# Plot histogram to check the distribution of age
plt.figure(figsize=(10, 6))
sns.histplot(data['Age'], bins=30, kde=True)
plt.show()

"""#### **Observations and Insights: _____**

### **Kids**
* Let's create feature "Kids" indicating the total kids and teens in the home.
"""

# Add Kidhome and Teenhome variables to create the new feature called "Kids"
data["Kids"] = data["Kidhome"] + data["Teenhome"]

"""### **Family Size**
* Let's create a new variable called 'Family Size' to find out how many members each family has.
* For this, we need to have a look at the Marital_Status variable, and see what are the categories.
"""

data['Family_Size'] = data['Kidhome'] + data['Teenhome'] + 2

"""* We can combine the sub-categories Single, Divorced, Widow as "Single" and we can combine the sub-categories Married and Together as "Relationship"
* Then we can create a new variable called "Status" and assign values 1 and 2 to categories Single and Relationship, respectively.
* Then, we can use the Kids (calculated above) and the Status column to find the family size.
"""

# Replace "Married" and "Together" with "Relationship"
data['Marital_Status'] = data['Marital_Status'].replace({'Married': 'Relationship', 'Together': 'Relationship'})

# Replace "Divorced" and "Widow" with "Single"
data['Marital_Status'] = data['Marital_Status'].replace({'Divorced': 'Single', 'Widow': 'Single'})

# Create a new feature called "Status" by replacing "Single" with 1 and "Relationship" with 2 in Marital_Status
data['Status'] = data['Marital_Status'].replace({'Single': 1, 'Relationship': 2})

# Add two variables Status and Kids to get the total number of persons in each family
data['Family_Size'] = data['Status'] + data['Kids']

"""### **Expenses**
* Let's create a new feature called "Expenses", indicating the total amount spent by the customers in various products over the span of two years.
"""

# Create a new feature
# Add the amount spent on each of product 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds'
data["Expenses"] = (data['MntWines'] + data['MntFruits'] + data['MntMeatProducts'] +
                    data['MntFishProducts'] + data['MntSweetProducts'] + data['MntGoldProds'])

"""### **Total Purchases**
* Let's create a new feature called "NumTotalPurchases", indicating the total number of products purchased by the customers.
"""

# Create a new feature
# Add the number of purchases from each channel 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases'
data["NumTotalPurchases"] = (data['NumDealsPurchases'] + data['NumWebPurchases'] +
                             data['NumCatalogPurchases'] + data['NumStorePurchases'])

"""### **Engaged in Days**
* Let's create a new feature called "Engaged in days", indicating how long the customer has been with the company.
"""

# Converting Dt_customer variable to Python date time object
data["Dt_Customer"] = pd.to_datetime(data["Dt_Customer"], format='%d-%m-%Y') # Specify the correct date format

"""**Let's check the max and min of the date.**"""

# Check the minimum of the date
# Hint: Use the min() method
min_date = data["Dt_Customer"].min()

# Check the maximum of the date
max_date = data["Dt_Customer"].max()

"""**Think About It:**
- From the above output from the max function, we observed that the last customer enrollment date is December 6th, 2014. Can we extract the number of days a customer has been with the company using some date as the threshold? Can January 1st, 2015 be that threshold?
"""

# Assigning date to the day variable
data["day"] = "01-01-2015"

# Converting the variable day to Python datetime object
data["day"] = pd.to_datetime(data.day)

data["Engaged_in_days"] = (data["day"] - data["Dt_Customer"]).dt.days

"""### **TotalAcceptedCmp**
* Let's create a new feature called "TotalAcceptedCmp" that shows how many offers customers have accepted.
"""

# Add all the campaign related variables to get the total number of accepted campaigns by a customer
# "AcceptedCmp1", "AcceptedCmp2", "AcceptedCmp3", "AcceptedCmp4", "AcceptedCmp5", "Response"
data["TotalAcceptedCmp"] = (data['AcceptedCmp1'] + data['AcceptedCmp2'] +
                            data['AcceptedCmp3'] + data['AcceptedCmp4'] +
                            data['AcceptedCmp5'] + data['Response'])

"""### **AmountPerPurchase**
* Let's create a new feature called "AmountPerPurchase" indicating the amount spent per purchase.
"""

# Divide the "Expenses" by "NumTotalPurchases" to create the new feature AmountPerPurchase
data["AmountPerPurchase"] = data["Expenses"] / data["NumTotalPurchases"]

"""**Now, let's check the maximum value of the AmountPerPurchase.**"""

# Check the max value
# Hint: Use max() function
max_value = data["AmountPerPurchase"].max()

"""**Think About It:**

- Is the maximum value in the above output valid? What could be the potential reason for such output?
- How many such values are there? Can we drop such observations?
"""

# Find how many observations have NumTotalPurchases equal to 0
num_zero_purchases = (data["NumTotalPurchases"] == 0).sum()

# Drop the observations with NumTotalPurchases equal to 0, using their indices
data = data[data["NumTotalPurchases"] != 0]

"""**Now, let's check the distribution of values in AmountPerPurchase column.**"""

# Check the summary statistics of the AmountPerPurchase variable
amount_per_purchase_stats = data["AmountPerPurchase"].describe()

# Plot the histogram for the AmountPerPurchas variable
plt.figure(figsize=(10, 5))
sns.histplot(data["AmountPerPurchase"], bins=30, kde=True)
plt.title("Distribution of AmountPerPurchase")
plt.xlabel("Amount Per Purchase")
plt.ylabel("Frequency")
plt.show()

"""#### **Observations and Insights: _____**

### **Imputing Missing Values**
"""

# Impute the missing values for the Income variable with the median
data["Income"].fillna(data["Income"].median(), inplace=True)

"""**Now that we are done with data preprocessing, let's visualize new features against the new income variable we have after imputing missing values.**

### **Income Vs Expenses**
"""

# Plot the scatter plot with Expenses on Y-axis and Income on X-axis

plt.figure(figsize=(20, 10))                                    # Setting the plot size

sns.scatterplot(x="Income", y="Expenses", data=data)                                        # Hint: Use sns.scatterplot()

plt.xticks(fontsize=16)                                         # Font size of X-label

plt.yticks(fontsize=16)                                         # Font size of Y-label

plt.xlabel("Income", fontsize=20, labelpad=20)                  # Title of X-axis

plt.ylabel("Expenses", fontsize=20, labelpad=20)                # Title of Y-axis

"""#### **Observations and Insights: _____**

### **Family Size Vs Income**
"""

# Assuming 'data' is your DataFrame and it is already loaded
plt.figure(figsize=(15, 7))
sns.barplot(x='Family_Size', y='Income', data=data)
plt.xlabel('Family Size', fontsize=20, labelpad=20)
plt.ylabel('Income', fontsize=20, labelpad=20)
plt.title('Family Size vs Income', fontsize=20, pad=20)
plt.show()



"""#### **Observations and Insights: _____**

## **Important Insights from EDA and Data Preprocessing**

What are the the most important observations and insights from the data based on the EDA and Data Preprocessing performed?

## Preparing Data for Segmentation

### Dropping columns that we will not use for segmentation

The decision about which variables to use for clustering is a critically important decision that will have a big impact on the clustering solution. So we need to think carefully about the variables we will choose for clustering. Clearly, this is a step where a lot of contextual knowledge, creativity, and experimentation/iterations are needed.

Moreover, we often use only a few of the data attributes for segmentation (the segmentation attributes) and use some of the remaining ones (the profiling attributes) only to profile the clusters. For example, in market research and market segmentation, we can use behavioral data for segmentation (to segment the customers based on their behavior like amount spent, units bought, etc.), and then use both demographic as well as behavioral data for profiling the segments found.

Here, we will use the behavioral attributes for segmentation and drop the demographic attributes like Income, Age, and Family_Size. In addition to this, we need to drop some other columns which are mentioned below.

* `Dt_Customer`: We have created the `Engaged_in_days` variable using the Dt_Customer variable. Hence, we can drop this variable as it will not help with segmentation.
* `Complain`: About 95% of the customers didn't complain and have the same value for this column. This variable will not have a major impact on segmentation. Hence, we can drop this variable.
* `day`:  We have created the `Engaged_in_days` variable using the 'day' variable. Hence, we can drop this variable as it will not help with segmentation.
* `Status`: This column was created just to get the `Family_Size` variable that contains the information about the Status. Hence, we can drop this variable.
* We also need to drop categorical variables like `Education` and `Marital_Status`, `Kids`, `Kidhome`, and `Teenhome` as distance-based algorithms cannot use the default distance like Euclidean to find the distance between categorical and numerical variables.
* We can also drop categorical variables like `AcceptedCmp1`, `AcceptedCmp2`, `AcceptedCmp3`, `AcceptedCmp4`, `AcceptedCmp5`, and `Response` for which we have create the variable `TotalAcceptedCmp` which is the aggregate of all these variables.
"""

# Dropping all the irrelevant columns and storing in data_model
data_model = data.drop(
    columns=[
        "Year_Birth",
        "Dt_Customer",
        "day",
        "Complain",
        "Response",
        "AcceptedCmp1",
        "AcceptedCmp2",
        "AcceptedCmp3",
        "AcceptedCmp4",
        "AcceptedCmp5",
        "Marital_Status",
        "Status",
        "Kids",
        'Education',
        'Kidhome',
        'Teenhome', 'Income','Age', 'Family_Size'
    ],
    axis=1,
)

# Check the shape of new data
data_model = data.drop(columns=[
    'Year_Birth', 'Dt_Customer', 'day', 'Complain', 'Response',
    'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5',
    'Marital_Status', 'Status', 'Kids', 'Education', 'Kidhome', 'Teenhome',
    'Income', 'Age', 'Family_Size'
])

# Check first five rows of new data
print(data_model.shape)
print(data_model.head())

"""**Let's plot the correlation plot after we've removed the irrelevant variables.**"""

# Plot the correlation plot for new data
plt.figure(figsize=(15, 7))
sns.heatmap(data_model.corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='Spectral')
plt.title('Correlation Heatmap')
plt.show()

"""**Observations and Insights:**

### Scaling the Data

**What is feature scaling?**

Feature scaling is a class of statistical techniques that, as the name implies, scales the features of our data so that they all have a similar range. You'll understand better if we look at an example:

If you have multiple independent variables like Age, Income, and Amount related variables, with their range as (18–100 Years), (25K–75K), and (100–200), respectively, feature scaling would help them all to be in the same range.

**Why feature scaling is important in Unsupervised Learning?**

Feature scaling is especially relevant in machine learning models that compute some sort of distance metric as we do in most clustering algorithms, for example, K-Means.

So, scaling should be done to avoid the problem of one feature dominating over others because the unsupervised learning algorithm uses distance to find the similarity between data points.

**Let's scale the data**

**Standard Scaler**: StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation.

![SC.png](attachment:SC.png)

1. Data standardization is the process of rescaling the attributes so that they have a mean of 0 and a variance of 1.
2. The ultimate goal to perform standardization is to bring down all the features to a common scale without distorting the differences in the range of the values.
3. In sklearn.preprocessing.StandardScaler(), centering and scaling happen independently on each feature.
"""

# Applying standard scaler on new data
scaler = StandardScaler()                                                   # Initialize the Standard Scaler

df_scaled = scaler.fit_transform(data_model)                                        # fit_transform the scaler function on new data

df_scaled = pd.DataFrame(df_scaled, columns=data_model.columns)      # Converting the embeddings to a dataframe

df_scaled.head()

"""## **Applying T-SNE and PCA to the data to visualize the data distributed in 2 dimensions**

### **Applying T-SNE**
"""

# Fitting T-SNE with number of components equal to 2 to visualize how data is distributed

tsne = TSNE(n_components=2, random_state=1, perplexity=35)        # Initializing T-SNE with number of component equal to 2, random_state=1, and perplexity=35

data_air_pol_tsne = tsne.fit_transform(df_scaled)                           # fit_transform T-SNE on new data

data_air_pol_tsne = pd.DataFrame(data_air_pol_tsne, columns=[0, 1])           # Converting the embeddings to a dataframe

plt.figure(figsize=(7, 7))                                                    # Scatter plot for two components

sns.scatterplot(x=0, y=1, data=data_air_pol_tsne)                             # Plotting T-SNE

"""**Observation and Insights:**

### **Applying PCA**

**Think about it:**
- Should we apply clustering algorithms on the current data or should we apply PCA on the data before applying clustering algorithms? How would this help?

When the variables used in clustering are highly correlated, it causes multicollinearity, which affects the clustering method and results in poor cluster profiling (or biased toward a few variables). PCA can be used to reduce the multicollinearity between the variables.
"""

# Defining the number of principal components to generate
n = data_model.shape[1]                                        # Storing the number of variables in the data

pca = PCA(n_components=n, random_state=1)                                        # Initialize PCA with n_components = n and random_state=1

data_pca = pd.DataFrame(pca.fit_transform(df_scaled))        # Apply PCA transformation to the scaled data and store the result

# The percentage of variance explained by each principal component is stored
exp_var = pca.explained_variance_ratio_

"""**Let's plot the first two components and see how the data points are distributed.**"""

plt.figure(figsize=(7, 7))
sns.scatterplot(x=0, y=1, data=data_pca)  # Use numerical column indices
plt.title('PCA - First two principal components')
plt.show()

"""**Let's apply clustering algorithms on the data generated after applying PCA**

## **K-Means**
"""

distortions = []                                                  # Create an empty list

K = range(2, 10)                                                  # Setting the K range from 2 to 10

for k in K:
    kmeanModel = KMeans(n_clusters=k,random_state=4)              # Initialize K-Means
    kmeanModel.fit(data_pca)                                      # Fit K-Means on the data
    distortions.append(kmeanModel.inertia_)                       # Append distortion values to the empty list created above

# Plotting the elbow plot
plt.figure(figsize=(16, 8))                                            # Setting the plot size

plt.plot(K, distortions, "bx-")                                        # Plotting the K on X-axis and distortions on y-axis

plt.xlabel("k")                                                        # Title of x-axis

plt.ylabel("Distortion")                                               # Title of y-axis

plt.title("The Elbow Method showing the optimal k")                    # Title of the plot
plt.show()

"""**In the above plot, the elbow is seen for K=3 and K=5 as there is some drop in distortion at K=3 and K=5.**

**Think About It:**

- How do we determine the optimal K value when the elbows are observed at 2 or more K values from the elbow curve?
- Which metric can be used to determine the final K value?

**We can use the silhouette score as a metric for different K values to make a better decision about picking the number of clusters(K).**

### **What is the silhouette score?**

Silhouette score is one of the methods for evaluating the quality of clusters created using clustering algorithms such as K-Means. The silhouette score is a measure of how similar an object is to its cluster (cohesion) compared to other clusters (separation). Silhouette score has a range of [-1, 1].

* Silhouette coefficients near +1 indicate that the clusters are dense and well separated, which is good.
* Silhouette score near -1 indicates that those samples might have been assigned to the wrong cluster.

**Finding silhouette score for each value of K**
"""

sil_score = []                                                             # Creating empty list
cluster_list = range(3, 7)                                                 # Creating a range from 3 to 7
for n_clusters in cluster_list:

    # Initialize K-Means with number of clusters equal to n_clusters and random_state=1
    clusterer =  KMeans(n_clusters=n_clusters, random_state=1)

    # Fit and predict on the pca data
    preds = clusterer.fit_predict(data_pca)

    # Calculate silhouette score - Hint: Use silhouette_score() function
    score = silhouette_score(data_pca, preds)

    # Append silhouette score to empty list created above
    sil_score.append(score)

    # Print the silhouette score
    print( "For n_clusters = {}, the silhouette score is {})".format(n_clusters, score))

"""**From the above silhouette scores, 3 appears to be a good value of K. So, let's build K-Means using K=3.**

### **Applying K-Means on data_pca**
"""

kmeans = KMeans(n_clusters=3, random_state=1)                                # Initialize the K-Means algorithm with 3 clusters and random_state=1

kmeans.fit(data_pca)                                   # Fitting on the data_pca

data_pca["K_means_segments_3"] = kmeans.labels_                    # Adding K-Means cluster labels to the data_pca data

data["K_means_segments_3"] = kmeans.labels_                        # Adding K-Means cluster labels to the whole data

data_model["K_means_segments_3"] = kmeans.labels_                  # Adding K-Means cluster labels to data_model

# Let's check the distribution
data_model["K_means_segments_3"].value_counts()

"""**Let's visualize the clusters using PCA**"""

# Function to visualize PCA data with clusters formed
def PCA_PLOT(X, Y, PCA, cluster):
    sns.scatterplot(x=X, y=1, data=PCA, hue=cluster)

PCA_PLOT(0, 1, data_pca, "K_means_segments_3")

"""**Observations and Insights:**

### **Cluster Profiling**
"""

cluster_profile_KMeans_3 = data.groupby('K_means_segments_3').mean()

# Highlighting the maximum average value among all the clusters for each of the variables
cluster_profile_KMeans_3.style.highlight_max(color="lightgreen", axis=0)

"""**Observations and Insights:**

**Let us create a boxplot for each of the variables**
"""

# Columns to use in boxplot
col_for_box = ['Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Complain','Age','Family_Size','Expenses','NumTotalPurchases','Engaged_in_days','TotalAcceptedCmp','AmountPerPurchase']

# Creating boxplot for each of the variables
all_col = col_for_box

plt.figure(figsize = (30, 50))

for i, variable in enumerate(all_col):
    plt.subplot(6, 4, i + 1)

    sns.boxplot(y=data[variable], x=data['K_means_segments_3'],showmeans=True)

    plt.tight_layout()

    plt.title(variable)

plt.show()

"""### **Characteristics of each cluster:**

**Cluster 0:__________**

**Summary for cluster 0:_______________**

**Cluster 1:_______________**

**Summary for cluster 1:_______________**



**Cluster 2:_______________**

**Summary for cluster 2:_______________**

**Think About It:**
- Are the K-Means profiles with K=3 providing any deep insights into customer purchasing behavior or which channels they are using?
- What is the next step to get more meaningful insights?

We can see from the above profiles that K=3 segments the customers into High, Medium and Low-income customers, and we are not getting deep insights into different types of customers. So, let's try to build K=5 (which has another elbow in the Elbow curve) and see if we can get better cluster profiles.
"""

# Dropping labels we got from K=3 since we will be using PCA data for prediction
# Drop K_means_segments_3. Hint: Use axis=1 and inplace=True
data_pca = data_pca.drop('K_means_segments_3', axis=1)
data_model = data_model.drop('K_means_segments_3', axis=1)

"""**Let's build K-Means using K=5**"""

# Fit the K-Means algorithm using number of cluster as 5 and random_state=0 on data_pca
kmeans = KMeans(n_clusters=5, random_state=0)
data_pca['K_means_segments_5'] = kmeans.fit_predict(data_pca)

# Add K-Means cluster labels to data_pca
data_pca['K_means_segments_5'] = kmeans.labels_

# Add K-Means cluster labels to whole data
data['K_means_segments_5'] = kmeans.labels_

# Add K-Means cluster labels to data_model
data_model['K_means_segments_5'] = kmeans.labels_

# Let's check the distribution
print(data_model['K_means_segments_5'].value_counts())

"""**Let's visualize the clusters using PCA**"""

# Define PCA plot function
def PCA_PLOT(X, y, PCA, cluster):
    sns.scatterplot(x=X, y=y, data=PCA, hue=cluster)
    plt.show()

# Visualize the clusters using PCA
PCA_PLOT(0, 1, data_pca, "K_means_segments_5")

"""### **Cluster Profiling**"""

# Take the cluster-wise mean of all the variables. Hint: First groupby 'data' by cluster labels column and then find mean
cluster_profile_KMeans_5 = data.groupby('K_means_segments_5').mean()

# Highlight the maximum average value among all the clusters for each of the variables
cluster_profile_KMeans_5.style.highlight_max(color='lightgreen', axis=0)

"""**Let's plot the boxplot**"""

# Create boxplot for each of the variables
col_for_box = ['Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits',
               'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',
               'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',
               'NumWebVisitsMonth', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4',
               'AcceptedCmp5', 'TotalAcceptedCmp', 'AmountPerPurchase']

"""### **Characteristics of each cluster**

**Cluster 0:__________**

**Summary for cluster 0:_______________**

**Cluster 1:_______________**

**Summary for cluster 1:_______________**


**Cluster 2:_______________**

**Summary for cluster 2:_______________**

**Cluster 3:_______________**

**Summary for cluster 3:_______________**

**Cluster 4:_______________**

**Summary for cluster 4:_______________**
"""

# Dropping labels we got from K-Means since we will be using PCA data for prediction
# Hint: Use axis=1 and inplace=True
data_pca.drop(columns=['K_means_segments_5'], axis=1, inplace=True)
data.drop(columns=['K_means_segments_5'], axis=1, inplace=True)

"""From the above profiles, K=5 provides more interesting insights about customer's purchasing behavior and preferred channels for purchasing products. We can also see that the High, Medium and Low income groups have different age groups and preferences, which was not evident in K=3. So, **we can choose K=5.**

## **K-Medoids**

**Let's find the silhouette score for K=5 in K-Medoids**
"""

kmedo = KMedoids(n_clusters=5, random_state=1)  # Initializing K-Medoids with number of clusters as 5 and random_state=1
preds = kmedo.fit_predict(data_pca)  # Fit and predict K-Medoids using data_pca
score = silhouette_score(data_pca, preds)  # Calculate the silhouette score
print(score)  # Print the score

"""**Observations and Insights:**"""

data_pca['K_medoids_segments_5'] = preds
data_model['K_medoids_segments_5'] = preds
data['K_medoids_segments_5'] = preds

# Let's check the distribution
print(data_model['K_medoids_segments_5'].value_counts())

"""**Let's visualize the clusters using PCA**"""

# Hint: Use PCA_PLOT function created above
def PCA_PLOT(x, y, PCA, cluster):
    sns.scatterplot(x=x, y=y, data=PCA, hue=cluster)

PCA_PLOT(0, 1, data_pca, 'K_medoids_segments_5')
plt.show()

"""### **Cluster Profiling**"""

# Take the cluster-wise mean of all the variables. Hint: First group 'data' by cluster labels column and then find mean
cluster_profile_KMeans_5 = data.groupby('K_medoids_segments_5').mean()

# Highlight the maximum average value among all the clusters for each of the variables
cluster_profile_KMeans_5.style.highlight_max(color='lightgreen', axis=0)

"""**Let's plot the boxplot**"""

# Create boxplot for each of the variables
plt.figure(figsize=(30, 50))
for i, variable in enumerate(col_for_box):
    plt.subplot(6, 4, i + 1)
    sns.boxplot(y=data[variable], x=data['K_medoids_segments_5'], showmeans=True)
    plt.tight_layout()
    plt.title(variable)
plt.show()

"""### **Characteristics of each cluster**

**Cluster 0:__________**

**Summary for cluster 0:_______________**

**Cluster 1:_______________**

**Summary for cluster 1:_______________**


**Cluster 2:_______________**

**Summary for cluster 2:_______________**

**Cluster 3:_______________**

**Summary for cluster 3:_______________**

**Cluster 4:_______________**

**Summary for cluster 4:_______________**
"""

# Dropping labels we got from K-Medoids since we will be using PCA data for prediction
# Hint: Use axis=1 and inplace=True
data_pca.drop(columns=['K_medoids_segments_5'], axis=1, inplace=True)
data.drop(columns=['K_medoids_segments_5'], axis=1, inplace=True)

"""## **Hierarchical Clustering**

Let's find the Cophenetic correlation for different distances with different linkage methods.
"""



"""### **What is a Cophenetic correlation?**

The cophenetic correlation coefficient is a correlation coefficient between the cophenetic distances(Dendrogramic distance) obtained from the tree, and the original distances used to construct the tree. It is a measure of how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points.

The cophenetic distance between two observations is represented in a dendrogram by the height of the link at which those two observations are first joined. That height is the distance between the two subclusters that are merged by that link.

Cophenetic correlation is the way to compare two or more dendrograms.

**Let's calculate Cophenetic correlation for each of the distance metrics with each of the linkage methods**
"""

# list of distance metrics
distance_metrics = ["euclidean", "chebyshev", "mahalanobis", "cityblock"]

# list of linkage methods
linkage_methods = ["single", "complete", "average"]

high_cophenet_corr = 0                                                 # Creating a variable by assigning 0 to it
high_dm_lm = [0, 0]                                                    # Creating a list by assigning 0's to it

for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(data_pca, metric=dm, method=lm)                    # Applying different linkages with different distance on data_pca
        c, coph_dists = cophenet(Z, pdist(data_pca))                   # Calculating cophenetic correlation
        print(
            "Cophenetic correlation for {} distance and {} linkage is {}.".format(
                dm.capitalize(), lm, c
            )
        )
        if high_cophenet_corr < c:                                     # Checking if cophenetic correlation is higher than previous score
            high_cophenet_corr = c                                     # Appending to high_cophenet_corr list if it is higher
            high_dm_lm[0] = dm                                         # Appending its corresponding distance
            high_dm_lm[1] = lm                                         # Appending its corresponding method or linkage

# Printing the combination of distance metric and linkage method with the highest cophenetic correlation
print(
    "Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.".format(
        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]
    )
)

"""**Let's have a look at the dendrograms for different linkages with `Cityblock distance`**"""

# List of linkage methods
linkage_methods = ["single", "complete", "average"]

# Lists to save results of cophenetic correlation calculation
compare_cols = ["Linkage", "Cophenetic Coefficient"]

# To create a subplot image
fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))            # Setting the plot size

# We will enumerate through the list of linkage methods above
# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation
for i, method in enumerate(linkage_methods):
    Z = linkage(data_pca, metric="Cityblock", method=method)                  # Measures the distances between two clusters

    dendrogram(Z, ax=axs[i])
    axs[i].set_title(f"Dendrogram ({method.capitalize()} Linkage)")           # Title of dendrogram

    coph_corr, coph_dist = cophenet(Z, pdist(data_pca))                       # Finding cophenetic correlation for different linkages with city block distance
    axs[i].annotate(
        f"Cophenetic\nCorrelation\n{coph_corr:0.2f}",
        (0.80, 0.80),
        xycoords="axes fraction",
    )

"""**Observations and Insights:**

**Think about it:**

- Can we clearly decide the number of clusters based on where to cut the dendrogram horizontally?
- What is the next step in obtaining number of clusters based on the dendrogram?

**Let's have a look at the dendrograms for different linkages with `Chebyshev distance`**
"""

# Plot the dendrogram for Chebyshev distance with linkages single, complete and average.
# Hint: Use Chebyshev distance as the metric in the linkage() function

linkage_methods = ["single", "complete", "average"]

fig, axes = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))

for i, method in enumerate(linkage_methods):
    Z = linkage(data_pca, metric='chebyshev', method=method)
    dendrogram(Z, ax=axes[i])
    axes[i].set_title(f"Dendrogram ({method.capitalize()} Linkage) with Chebyshev Distance")
    coph_corr, coph_dist = cophenet(Z, pdist(data_pca, metric='chebyshev'))
    axes[i].annotate(
        f"Cophenetic Correlation: {coph_corr:.2f}",
        xy=(0.80, 0.80),
        xycoords='axes fraction',
    )

plt.tight_layout()
plt.show()

"""**Observations and Insights:**

**Let's have a look at the dendrograms for different linkages with Mahalanobis distance**
"""

# Plot the dendrogram for Mahalanobis distance with linkages single, complete and average.
# Hint: Use Mahalanobis distance as the metric in the linkage() function
# Compute the covariance matrix of the data
cov_matrix = np.cov(data_pca, rowvar=False)
inv_cov_matrix = np.linalg.inv(cov_matrix)

# Define a custom Mahalanobis distance function
def mahalanobis_distance(u, v):
    delta = u - v
    return np.sqrt(np.dot(np.dot(delta, inv_cov_matrix), delta.T))

# Compute the pairwise Mahalanobis distances
mahalanobis_dists = pdist(data_pca, metric=mahalanobis_distance)

# Plotting the dendrogram for Mahalanobis distance with linkage methods single, complete, and average
linkage_methods = ["single", "complete", "average"]

fig, axes = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))

for i, method in enumerate(linkage_methods):
    Z = linkage(mahalanobis_dists, method=method)
    dendrogram(Z, ax=axes[i])
    axes[i].set_title(f"Dendrogram ({method.capitalize()} Linkage) with Mahalanobis Distance")
    coph_corr, coph_dist = cophenet(Z, mahalanobis_dists)
    axes[i].annotate(
        f"Cophenetic Correlation: {coph_corr:.2f}",
        xy=(0.80, 0.80),
        xycoords='axes fraction',
    )

plt.tight_layout()
plt.show()

"""**Observations and Insights:**

**Let's have a look at the dendrograms for different linkages with Euclidean distance**
"""

# Plot the dendrogram for Euclidean distance with linkages single, complete, average and ward.
# Hint: Use Euclidean distance as the metric in the linkage() function
# List of linkage methods
linkage_methods = ["single", "complete", "average", "ward"]

# Compute pairwise Euclidean distances
euclidean_dists = pdist(data_pca, metric='euclidean')

# Plotting the dendrogram for Euclidean distance with different linkage methods
fig, axes = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))

for i, method in enumerate(linkage_methods):
    Z = linkage(euclidean_dists, method=method)
    dendrogram(Z, ax=axes[i])
    axes[i].set_title(f"Dendrogram ({method.capitalize()} Linkage) with Euclidean Distance")
    coph_corr, coph_dist = cophenet(Z, euclidean_dists)
    axes[i].annotate(
        f"Cophenetic Correlation: {coph_corr:.2f}",
        xy=(0.80, 0.80),
        xycoords='axes fraction',
    )

plt.tight_layout()
plt.show()

"""**Think about it:**

- Are there any distinct clusters in any of the dendrograms?

**Observations and Insights:**
"""

# Initialize Agglomerative Clustering with affinity (distance) as Euclidean, linkage as 'Ward' with clusters=3
HCmodel = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')


# Fit on data_pca
HCmodel.fit(data_pca)

# Add Agglomerative Clustering cluster labels to data_pca
data_pca['HC_segments_3'] = HCmodel.labels_

# Add Agglomerative Clustering cluster labels to the whole data
data['HC_segments_3'] = HCmodel.labels_

# Add Agglomerative Clustering cluster labels to data_model
data_model['HC_segments_3'] = HCmodel.labels_

# Let's check the distribution
# Check the distribution of the clusters
data_model['HC_segments_3'].value_counts()

"""**Let's visualize the clusters using PCA.**"""

# Define the PCA_PLOT function
def PCA_PLOT(x, y, PCA, cluster):
    sns.scatterplot(x=x, y=y, data=PCA, hue=cluster)
# Visualize the clusters using PCA
PCA_PLOT(0, 1, data_pca, "HC_segments_3")

"""### **Cluster Profiling**"""

# Take the cluster-wise mean of all the variables. Hint: First group 'data' by cluster labels column and then find mean
cluster_profile_HC_3 = data_model.groupby('HC_segments_3').mean()

# Highlight the maximum average value among all the clusters for each of the variables
cluster_profile_HC_3.style.highlight_max(color='lightgreen', axis=0)

"""**Let's plot the boxplot**"""

# Create boxplot for each of the variables
# Create boxplot for each of the variables
col_for_box = ['Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']

plt.figure(figsize=(30, 50))
for i, variable in enumerate(col_for_box):
    plt.subplot(6, 4, i + 1)
    sns.boxplot(y=data[variable], x=data['HC_segments_3'], showmeans=True)
    plt.tight_layout()
    plt.title(variable)
plt.show()

"""### **Characteristics of each cluster**

**Cluster 0:__________**

**Summary for cluster 0:_______________**

**Cluster 1:_______________**

**Summary for cluster 1:_______________**


**Cluster 2:_______________**

**Summary for cluster 2:_______________**

**Observations and Insights:**
"""

# Dropping labels we got from Agglomerative Clustering since we will be using PCA data for prediction
# Hint: Use axis=1 and inplace=True
data_pca.drop('HC_segments_3', axis=1, inplace=True)
data.drop('HC_segments_3', axis=1, inplace=True)

"""## **DBSCAN**

DBSCAN is a very powerful algorithm for finding high-density clusters, but the problem is determining the best set of hyperparameters to use with it. It includes two hyperparameters, `eps`, and `min samples`.

Since it is an unsupervised algorithm, you have no control over it, unlike a supervised learning algorithm, which allows you to test your algorithm on a validation set. The approach we can follow is basically trying out a bunch of different combinations of values and finding the silhouette score for each of them.
"""

# Initializing lists
eps_value = [2,3]                       # Taking random eps value
min_sample_values = [6,20]              # Taking random min_sample value

# Creating a dictionary for each of the values in eps_value with min_sample_values
res = {eps_value[i]: min_sample_values for i in range(len(eps_value))}

# Finding the silhouette_score for each of the combinations
high_silhouette_avg = 0                                               # Assigning 0 to the high_silhouette_avg variable
high_i_j = [0, 0]                                                     # Assigning 0's to the high_i_j list
key = res.keys()                                                      # Assigning dictionary keys to a variable called key
for i in key:
    z = res[i]                                                        # Assigning dictionary values of each i to z
    for j in z:
        db = DBSCAN(eps=i, min_samples=j).fit(data_pca)               # Applying DBSCAN to each of the combination in dictionary
        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
        core_samples_mask[db.core_sample_indices_] = True
        labels = db.labels_
        silhouette_avg = silhouette_score(data_pca, labels)           # Finding silhouette score
        print(
            "For eps value =" + str(i),
            "For min sample =" + str(j),
            "The average silhoutte_score is :",
            silhouette_avg,                                          # Printing the silhouette score for each of the combinations
        )
        if high_silhouette_avg < silhouette_avg:                     # If the silhouette score is greater than 0 or the previous score, it will get appended to the high_silhouette_avg list with its combination of i and j
            high_i_j[0] = i
            high_i_j[1] = j

# Printing the highest silhouette score
print("Highest_silhoutte_avg is {} for eps = {} and min sample = {}".format(high_silhouette_avg, high_i_j[0], high_i_j[1]))

"""**Now, let's apply DBSCAN using the hyperparameter values we have received above.**"""

# Apply DBSCAN using the above hyperparameter values
dbs = DBSCAN(eps=high_i_j[0], min_samples=high_i_j[1])

# Fit and predict on data_pca and add DBSCAN cluster labels to the whole data
data['DBSCAN_segments'] = dbs.fit_predict(data_pca)

# Fit and predict on data_pca and add DBSCAN cluster labels to data_model
data_model['DBSCAN_segments'] = dbs.fit_predict(data_pca)

# Fit and predict on data_pca and add DBSCAN cluster labels to data_pca
data_pca['DBSCAN_segments'] = dbs.fit_predict(data_pca)

# Let's check the distribution
print(data['DBSCAN_segments'].value_counts())

"""**Let's visualize the clusters using PCA.**"""

# Hint: Use PCA_PLOT function created above
PCA_PLOT(0, 1, data_pca, 'DBSCAN_segments')

"""**Observations and Insights:**

**Think about it:**

- Changing the eps and min sample values will result in different DBSCAN results? Can we try more value for eps and min_sample?

**Note:** You can experiment with different eps and min_sample values to see if DBSCAN produces good distribution and cluster profiles.
"""

# Dropping labels we got from DBSCAN since we will be using PCA data for prediction
# Hint: Use axis=1 and inplace=True
data_pca.drop(columns=['DBSCAN_segments'], axis=1, inplace=True)
data.drop(columns=['DBSCAN_segments'], axis=1, inplace=True)

"""## **Gaussian Mixture Model**

**Let's find the silhouette score for K=5 in Gaussian Mixture**
"""

gmm = GaussianMixture(n_components=5, random_state=1)# Initialize Gaussian Mixture Model with number of clusters as 5 and random_state=1

preds = gmm.fit_predict(data_pca)            # Fit and predict Gaussian Mixture Model using data_pca

score = silhouette_score(data_pca, preds)           # Calculate the silhouette score

print(score)                   # Print the score

"""**Observations and Insights:**"""

# Predicting on data_pca and add Gaussian Mixture Model cluster labels to the whole data
data_pca['GMM_segments'] = preds
data['GMM_segments'] = preds
data_model['GMM_segments'] = preds

# Let's check the distribution
data_model['GMM_segments'].value_counts()

"""**Let's visualize the clusters using PCA.**"""

# Hint: Use PCA_PLOT function created above
def PCA_PLOT(x, y, PCA, cluster):
    sns.scatterplot(x=x, y=y, data=PCA, hue=cluster)

PCA_PLOT(0, 1, data_pca, "GMM_segments")

"""### **Cluster Profiling**"""

# Take the cluster-wise mean of all the variables. Hint: First group 'data' by cluster labels column and then find mean
cluster_profile_GMM = data_model.groupby('GMM_segments').mean()

# Highlight the maximum average value among all the clusters for each of the variables
cluster_profile_GMM.style.highlight_max(color="lightgreen", axis=0)

"""**Let's plot the boxplot**"""

# Create boxplot for each of the variables
col_for_box = ['Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',
               'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',
               'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']

plt.figure(figsize=(30, 50))

for i, variable in enumerate(col_for_box):
    plt.subplot(6, 4, i + 1)
    # Check if the column exists in the DataFrame before plotting
    if variable in data_model.columns:
        sns.boxplot(y=data_model[variable], x=data_model['GMM_segments'], showmeans=True)
        plt.tight_layout()
        plt.title(variable)
    else:
        print(f"Column '{variable}' not found in DataFrame.")

plt.show()

"""### **Characteristics of each cluster**

**Cluster 0:__________**

**Summary for cluster 0:_______________**

**Cluster 1:_______________**

**Summary for cluster 1:_______________**


**Cluster 2:_______________**

**Summary for cluster 2:_______________**

**Cluster 3:_______________**

**Summary for cluster 3:_______________**

**Cluster 4:_______________**

**Summary for cluster 4:_______________**

## **Conclusion and Recommendations**

**1. Comparison of various techniques and their relative performance based on chosen Metric (Measure of success)**:
- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?

**2. Refined insights**:
- What are the most meaningful insights from the data relevant to the problem?

**3. Proposal for the final solution design:**
- What model do you propose to be adopted? Why is this the best solution to adopt?
"""